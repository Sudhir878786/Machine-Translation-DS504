{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef57a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dd060c",
   "metadata": {},
   "source": [
    "The many-to-many sequence modelling technique known as seq2seq architecture is frequently used for a number of applications including text summarization, chatbot generation, conversational modelling, and neural machine translation, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0bb85c",
   "metadata": {},
   "source": [
    "We'll look at how to build a language translation model, which is another well-known use for neural machine translation. Using Python's Keras library, we will construct our language translation model using the seq2seq architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b7da5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set values for different parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LSTM_NODES =256\n",
    "NUM_SENTENCES = 20000\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e3a95",
   "metadata": {},
   "source": [
    "The model of language translation we'll create in this article will convert English sentences into their French equivalents. We require a dataset with English sentences and their French translations in order to create such a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d846aa6",
   "metadata": {},
   "source": [
    "This file is \"fra.txt\". On each line, the text file contains an English sentence and its French translation, separated by a tab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d36228f",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe6a36",
   "metadata": {},
   "source": [
    "The seq2seq architecture is frequently used as the basis for neural machine translation models. The encoder LSTM and the decoder LSTM networks make up the encoder-decoder architecture known as the seq2seq. The sentence in the original language serves as the input for the encoder LSTM, and the sentence in the translated language along with a start-of-sentence token serves as the input for the decoder LSTM. With a token at the end of the sentence, the output is the actual target sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad205f",
   "metadata": {},
   "source": [
    "In our dataset, we do not need to process the input, however, we need to generate two copies of the translated sentence: one with the start-of-sentence token and the other with the end-of-sentence token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "126fccda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples input: 19974\n",
      "num samples output: 19974\n",
      "num samples output input: 19974\n"
     ]
    }
   ],
   "source": [
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "# there are three lists input_sentences[], output_sentences[], and output_sentences_inputs[]. \n",
    "\n",
    "count = 0\n",
    "# in the for loop the fra.txt file is read line by line. \n",
    "for line in open('fra.txt', encoding=\"utf-8\"):\n",
    "#     Each line is split into two substrings at the position where the tab occurs. \n",
    "# The left substring (the English sentence) is inserted into the input_sentences[] list. \n",
    "# The substring to the right of the tab is the corresponding translated French sentence.\n",
    "    count += 1\n",
    "\n",
    "    if count > NUM_SENTENCES:\n",
    "        break\n",
    "\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "\n",
    "    input_sentence, output = line.rstrip().split('\\t')\n",
    "#     The <eos> token, which marks the end-of-sentence is prefixed to the translated sentence, \n",
    "# and the resultant sentence is appended to the output_sentences[] list. \n",
    "# Similarly, the <sos> token, which stands for \"start of sentence\", \n",
    "# is concatenated at the start of the translated sentence and the result is added to the output_sentences_inputs[] list.\n",
    "    output_sentence = output + ' <eos>'\n",
    "    output_sentence_input = '<sos> ' + output\n",
    "\n",
    "    input_sentences.append(input_sentence)\n",
    "    output_sentences.append(output_sentence)\n",
    "    output_sentences_inputs.append(output_sentence_input)\n",
    "# The loop terminates if the number of sentences added to the lists is greater than the NUM_SENTENCES variable, i.e. 20,000.\n",
    "\n",
    "print(\"num samples input:\", len(input_sentences))\n",
    "print(\"num samples output:\", len(output_sentences))\n",
    "print(\"num samples output input:\", len(output_sentences_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396427a2",
   "metadata": {},
   "source": [
    "we will only use the first 20,000 records to train our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a0984a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be nice.\n",
      "Sois gentille ! <eos>\n",
      "<sos> Sois gentille !\n"
     ]
    }
   ],
   "source": [
    "# randomly print a sentence from the input_sentences[], output_sentences[], and output_sentences_inputs[] lists:\n",
    "print(input_sentences[172])\n",
    "print(output_sentences[172])\n",
    "print(output_sentences_inputs[172])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c02ad",
   "metadata": {},
   "source": [
    "You can see the original sentence, i.e. Be nice.; its corresponding translation in the output, i.e Sois gentille ! <eos>. <eos>. Notice, here we have <eos> token at the end of the sentence. Similarly, for the input to the decoder, we have <sos> Sois gentille ! <eos>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd3f998",
   "metadata": {},
   "source": [
    "**Tokenization and Padding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8686740",
   "metadata": {},
   "source": [
    "After tokenizing the original and translated sentences, padding is applied to any sentences that are either too long or too short. In the case of inputs, this padding will be equal to the length of the longest input sentence. Additionally, the longest sentence in the output will be this length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2711f20f",
   "metadata": {},
   "source": [
    "The Tokenizer class from the keras.preprocessing.text package can be used for tokenization. The tokenizer class carries out two functions: It breaks a sentence up into its component words, then turns those words into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a88d5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 3426\n",
      "Length of longest sentence in input: 5\n"
     ]
    }
   ],
   "source": [
    "# to tokenize the input sentences\n",
    "\n",
    "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Length of longest sentence in input: %g\" % max_input_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1908454b",
   "metadata": {},
   "source": [
    "In addition to tokenization and integer conversion, the word_index attribute of the Tokenizer class returns a word-to-index dictionary where words are the keys and the corresponding integers are the values. The script above also prints the number of unique words in the dictionary and the length of the longest sentence in the input:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c565d5",
   "metadata": {},
   "source": [
    "Similarly, the output sentences can also be tokenized in the same way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "194ae896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the output: 9504\n",
      "Length of longest sentence in the output: 12\n"
     ]
    }
   ],
   "source": [
    "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "output_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Length of longest sentence in the output: %g\" % max_out_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d8b81",
   "metadata": {},
   "source": [
    "English sentences are typically shorter and include fewer words on average than the translated French sentences, according to a comparison of the number of unique words in the input and the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6837f5",
   "metadata": {},
   "source": [
    "The input has to be padded next. Because text sentences can be of different lengths, but LSTM (the algorithm we will use to train our model) expects input instances with the same length, padding is used for both the input and the output. Because of this, we must turn our sentences into fixed-length vectors. Padding is one method for achieving this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08a0b8",
   "metadata": {},
   "source": [
    "A specific sentence length is established in padding. In our example, the input and output sentences will be padded by the length of the longest sentence from the inputs and outputs, respectively. The input's longest sentence is six words long. Zeros will be added to the empty indexes for sentences with fewer than six words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45975dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_sequences.shape: (19974, 5)\n",
      "encoder_input_sequences[172]: [  0   0   0  22 114]\n"
     ]
    }
   ],
   "source": [
    "# to apply padding to the input sentences\n",
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
    "print(\"encoder_input_sequences[172]:\", encoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa92b25",
   "metadata": {},
   "source": [
    "The script above prints the shape of the padded input sentences. The padded integer sequence for the sentence at index 172 is also printed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7b9d28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_sequences.shape: (19974, 12)\n",
      "decoder_input_sequences[172]: [  2  62 783   4   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# In the same way, the decoder outputs and the decoder inputs are padded\n",
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
    "print(\"decoder_input_sequences[172]:\", decoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810066bb",
   "metadata": {},
   "source": [
    "It is also crucial to note that the decoder applies post-padding, which results in the addition of zeros at the end of the phrase. Zeros were paddinged at the start of the encoder. This method was chosen because encoder output is based on words that appear at the end of sentences, so the original words were left in place there and zeros were added to the beginning. The decoder, on the other hand, begins processing at the beginning of a sentence, so post-padding is applied to the decoder inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b2a8d",
   "metadata": {},
   "source": [
    "**Word Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeb1c9c",
   "metadata": {},
   "source": [
    "We must transform our words into their corresponding numeric vector representations because we are using deep learning models, and deep learning models only work with numbers. However, we have already transformed our words into integers. What distinguishes word embeddings from integer representation, then?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510f25f",
   "metadata": {},
   "source": [
    "Word embeddings and single integer representations differ primarily in two ways. A word is only represented by a single integer in integer representation. A word is represented as a vector in vector representation, which can have any number of dimensions—50, 100, 200, etc. Word embeddings therefore record a great deal more information about words. Second, the links between various words are not represented by the single-integer representation. Word embeddings, on the other hand, preserve the connections between the words. You have two options: pretrained word embeddings or custom word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61829af4",
   "metadata": {},
   "source": [
    "Let's create word embeddings for the inputs first. To do so, we need to load the GloVe word vectors into memory. We will then create a dictionary where words are the keys and the corresponding vectors are values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3693525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9283f52d",
   "metadata": {},
   "source": [
    "The integer value of the word will be represented by the row number in the matrix, and the word's dimensions will be represented by the columns. The word embeddings for the words in our input sentences are contained in this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da7a7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = zeros((num_words, EMBEDDING_SIZE))\n",
    "for word, index in word2idx_inputs.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5e5b63",
   "metadata": {},
   "source": [
    "This word embedding matrix will be used to create the embedding layer for our LSTM model.\n",
    "The following script creates the embedding layer for the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "939489fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628427f3",
   "metadata": {},
   "source": [
    "**Creating the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60ec72",
   "metadata": {},
   "source": [
    " The first thing we need to do is to define our outputs, as we know that the output will be a sequence of words. \n",
    " for each input sentence, we need a corresponding output sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fd49780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the empty output array\n",
    "decoder_targets_one_hot = np.zeros((\n",
    "        len(input_sentences),\n",
    "        max_out_len,\n",
    "        num_words_output\n",
    "    ),\n",
    "    dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dfaa76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19974, 12, 9505)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to print the shape of decoder\n",
    "decoder_targets_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9546894",
   "metadata": {},
   "source": [
    "The final layer of the model, which will be a dense layer for making predictions, requires outputs in the form of one-hot encoded vectors because the dense layer will use the softmax activation function. The next step is to assign 1 to the column number that corresponds to the word's integer representation in order to produce such one-hot encoded output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aec2371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a93e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(decoder_output_sequences):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913bac1",
   "metadata": {},
   "source": [
    "Next, we need to create the encoder and decoders. The input to the encoder will be the sentence in English and the output will be the hidden state and cell state of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc37fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script defines the encoder\n",
    "encoder_inputs_placeholder = Input(shape=(max_input_len,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(LSTM_NODES, return_state=True)\n",
    "\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "encoder_states = [h, c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca93692",
   "metadata": {},
   "source": [
    "The next step is to define the decoder. The decoder will have two inputs: the hidden state and cell state from the encoder and the input sentence, which actually will be the output sentence with an <sos> token appended at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb20a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_placeholder = Input(shape=(max_out_len,))\n",
    "\n",
    "decoder_embedding = Embedding(num_words_output, LSTM_NODES)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "decoder_lstm = LSTM(LSTM_NODES, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2340574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output from the decoder LSTM is passed through a dense layer to predict decoder outputs\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af1eb27",
   "metadata": {},
   "source": [
    "compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ca32d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs_placeholder,\n",
    "  decoder_inputs_placeholder], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59034892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\users\\amar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\amar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydot) (2.4.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\AMAR\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a6c77e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\amar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\AMAR\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff553885",
   "metadata": {},
   "source": [
    "we have two types of input. input_1 is the input placeholder for the encoder, which is embedded and passed through lstm_1 layer, which basically is the encoder LSTM. There are three outputs from the lstm_1 layer: the output, the hidden layer and the cell state. However, only the cell state and the hidden state are passed to the decoder.\n",
    "\n",
    "Here the lstm_2 layer is the decoder LSTM. The input_2 contains the output sentences with <sos> token appended at the start. The input_2 is also passed through an embedding layer and is used as input to the decoder LSTM, lstm_2. Finally, the output from the decoder LSTM is passed through the dense layer to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e50a0189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "281/281 [==============================] - 106s 358ms/step - loss: 2.0548 - accuracy: 0.7173 - val_loss: 2.0620 - val_accuracy: 0.7081\n",
      "Epoch 2/20\n",
      "281/281 [==============================] - 81s 290ms/step - loss: 1.5143 - accuracy: 0.7761 - val_loss: 1.8286 - val_accuracy: 0.7379\n",
      "Epoch 3/20\n",
      "281/281 [==============================] - 76s 272ms/step - loss: 1.3232 - accuracy: 0.8050 - val_loss: 1.7141 - val_accuracy: 0.7560\n",
      "Epoch 4/20\n",
      "281/281 [==============================] - 77s 273ms/step - loss: 1.2079 - accuracy: 0.8192 - val_loss: 1.6486 - val_accuracy: 0.7641\n",
      "Epoch 5/20\n",
      "281/281 [==============================] - 77s 274ms/step - loss: 1.1179 - accuracy: 0.8293 - val_loss: 1.5941 - val_accuracy: 0.7710\n",
      "Epoch 6/20\n",
      "281/281 [==============================] - 72s 257ms/step - loss: 1.0444 - accuracy: 0.8378 - val_loss: 1.5785 - val_accuracy: 0.7714\n",
      "Epoch 7/20\n",
      "281/281 [==============================] - 76s 271ms/step - loss: 0.9866 - accuracy: 0.8453 - val_loss: 1.5566 - val_accuracy: 0.7723\n",
      "Epoch 8/20\n",
      "281/281 [==============================] - 74s 265ms/step - loss: 0.9382 - accuracy: 0.8515 - val_loss: 1.5483 - val_accuracy: 0.7736\n",
      "Epoch 9/20\n",
      "281/281 [==============================] - 102s 363ms/step - loss: 0.8934 - accuracy: 0.8584 - val_loss: 1.5515 - val_accuracy: 0.7735\n",
      "Epoch 10/20\n",
      "281/281 [==============================] - 91s 323ms/step - loss: 0.8555 - accuracy: 0.8637 - val_loss: 1.5402 - val_accuracy: 0.7777\n",
      "Epoch 11/20\n",
      "281/281 [==============================] - 93s 330ms/step - loss: 0.8199 - accuracy: 0.8686 - val_loss: 1.5509 - val_accuracy: 0.7754\n",
      "Epoch 12/20\n",
      "281/281 [==============================] - 76s 272ms/step - loss: 0.7918 - accuracy: 0.8734 - val_loss: 1.5639 - val_accuracy: 0.7767\n",
      "Epoch 13/20\n",
      "281/281 [==============================] - 75s 269ms/step - loss: 0.7647 - accuracy: 0.8775 - val_loss: 1.5681 - val_accuracy: 0.7777\n",
      "Epoch 14/20\n",
      "281/281 [==============================] - 86s 307ms/step - loss: 0.7361 - accuracy: 0.8818 - val_loss: 1.5602 - val_accuracy: 0.7764\n",
      "Epoch 15/20\n",
      "281/281 [==============================] - 85s 303ms/step - loss: 0.7144 - accuracy: 0.8854 - val_loss: 1.5727 - val_accuracy: 0.7787\n",
      "Epoch 16/20\n",
      "281/281 [==============================] - 81s 290ms/step - loss: 0.6959 - accuracy: 0.8891 - val_loss: 1.5844 - val_accuracy: 0.7804\n",
      "Epoch 17/20\n",
      "281/281 [==============================] - 73s 261ms/step - loss: 0.6827 - accuracy: 0.8917 - val_loss: 1.5992 - val_accuracy: 0.7801\n",
      "Epoch 18/20\n",
      "281/281 [==============================] - 79s 282ms/step - loss: 0.6700 - accuracy: 0.8940 - val_loss: 1.6100 - val_accuracy: 0.7811\n",
      "Epoch 19/20\n",
      "281/281 [==============================] - 77s 275ms/step - loss: 0.6599 - accuracy: 0.8964 - val_loss: 1.6313 - val_accuracy: 0.7788\n",
      "Epoch 20/20\n",
      "281/281 [==============================] - 83s 294ms/step - loss: 0.6491 - accuracy: 0.8983 - val_loss: 1.6430 - val_accuracy: 0.7812\n"
     ]
    }
   ],
   "source": [
    "#  train the model using the fit() method\n",
    "r = model.fit(\n",
    "    [encoder_input_sequences, decoder_input_sequences],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec745374",
   "metadata": {},
   "source": [
    "The model is trained on 18,000 records and tested on the remaining 2,000 records. The model is trained for 20 epochs, you can modify the number of epochs to see if you can get better results. After 20 epochs, I got training accuracy of 89.83% and the validation accuracy of 78.12% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fa4807",
   "metadata": {},
   "source": [
    "**Modifying the Model for Predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72e2ff",
   "metadata": {},
   "source": [
    "The encoder model remains the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97a2c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs_placeholder, encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d01537",
   "metadata": {},
   "source": [
    "Since now at each step we need the decoder hidden and cell states, we will modify our model to accept the hidden and cell states as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9ffb768",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(LSTM_NODES,))\n",
    "decoder_state_input_c = Input(shape=(LSTM_NODES,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017d3fd",
   "metadata": {},
   "source": [
    "Now at each time step, there will be only single word in the decoder input, we need to modify the decoder embedding layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "371fc893",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb6d73",
   "metadata": {},
   "source": [
    "Next, we need to create the placeholder for decoder outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b5354ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed1dba8",
   "metadata": {},
   "source": [
    "To make predictions, the decoder output is passed through the dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed2038f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_states = [h, c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c91f5",
   "metadata": {},
   "source": [
    "The final step is to define the updated decoder model, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9059eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ecafc0",
   "metadata": {},
   "source": [
    "**Making Predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b1a51",
   "metadata": {},
   "source": [
    "Words were transformed into integers throughout the tokenization processes. The decoder will also produce integer outputs. On the other hand, we need a string of French words as our output. We must do this by changing the integers back to words. For both inputs and outputs, we will create new dictionaries with words as the corresponding values and integers as the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1260f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae91350",
   "metadata": {},
   "source": [
    "Next we will create a method, i.e. translate_sentence(). The method will accept an input-padded sequence English sentence (in the integer form) and will return the translated French sentence. Look at the translate_sentence() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aab62b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(input_seq):\n",
    "#     we pass the input sequence to the encoder_model, \n",
    "# which predicts the hidden state and the cell state, which are stored in the states_value variable.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "#     we define a variable target_seq, which is a 1 x 1 matrix of all zeros. \n",
    "# The target_seq variable contains the first word to the decoder model, which is <sos>.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "#     After that, the eos variable is initialized, which stores the integer value for the <eos> token. \n",
    "# In the next line, the output_sentence list is defined, which will contain the predicted translation.\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "    output_sentence = []\n",
    "\n",
    "    '''\n",
    "    Next, we execute a for loop. \n",
    "    The number of execution cycles for the for loop is equal to the length of the longest sentence in the output. \n",
    "    Inside the loop, in the first iteration, the decoder_model predicts the output and the hidden and cell states, \n",
    "    using the hidden and cell state of the encoder, and the input token, i.e. <sos>. The index of the predicted word is \n",
    "    stored in the idx variable. If the value of the predicted index is equal to the <eos> token, the loop terminates. \n",
    "    Else if the predicted index is greater than zero, the corresponding word is retrieved from the idx2word dictionary and \n",
    "    is stored in the word variable, which is then appended to the output_sentence list. The states_value variable is updated\n",
    "    with the new hidden and cell state of the decoder and the index of the predicted word is stored in the target_seq variable. \n",
    "    In the next loop cycle, the updated hidden and cell states, along with the index of the previously predicted word, \n",
    "    are used to make new predictions. \n",
    "    The loop continues until the maximum output sequence length is achieved or the <eos> token is encountered.\n",
    "    '''\n",
    "    \n",
    "    for _ in range(max_out_len):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "\n",
    "        if idx > 0:\n",
    "            word = idx2word_target[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        target_seq[0, 0] = idx\n",
    "        states_value = [h, c]\n",
    "# Finally, the words in the output_sentence \n",
    "# list are concatenated using a space and the resulting string is returned to the calling function.\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06972d5e",
   "metadata": {},
   "source": [
    "**Testing the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffebfc86",
   "metadata": {},
   "source": [
    "To test the code, we will randomly choose a sentence from the input_sentences list, retrieve the corresponding padded sequence for the sentence, and will pass it to the translate_sentence() method. The method will return the translated sentence as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68960507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input: They just left.\n",
      "Response: ils sont partir.\n"
     ]
    }
   ],
   "source": [
    "# test the functionality of the model\n",
    "i = np.random.choice(len(input_sentences))\n",
    "input_seq = encoder_input_sequences[i:i+1]\n",
    "translation = translate_sentence(input_seq)\n",
    "print('-')\n",
    "print('Input:', input_sentences[i])\n",
    "print('Response:', translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa77f99",
   "metadata": {},
   "source": [
    "The model has successfully translated another English sentence into French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46389ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
